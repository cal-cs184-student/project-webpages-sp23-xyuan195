<html>
	<head>
		<link rel="stylesheet" type="text/css" href="semantic/semantic.min.css">
		<script
				src="https://code.jquery.com/jquery-3.1.1.min.js"
				integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="
				crossorigin="anonymous"></script>
		<script src="semantic/semantic.min.js"></script>
		<title>Assignment 1 Write-up (xyuan195)</title>
	</head>
	<body>
		<div class="ui container">
			<div class="ui center aligned container">
				<br>
				<h1>Assignment 1 (Rasterizer) Write-up</h1>
				<h4>Author: Xiaoyuan Zhu (m195@berkeley.edu)</h4>
				<h4><a href="https://github.com/cal-cs184-student/p1-rasterizer-sp23-xyuan195">Project Github Repo</a></h4>
			</div>
			
			<h2>Overview</h2>
			<p>
				In this assignment, I explored different techniques for rasterizing objects, antialiasing, and texture mapping.
				These techniques include supersampling, pixel sampling for texture mapping, level sampling for texture mapping, etc.
				I also compared these techniques for their memory usage, speed, and antialiasing effect.
			</p>
			<h2>Tasks Breakdown</h2>
			<h3>Task 1: Drawing Single-Color Triangles (<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">Github Page</a>)</h3>
			<h4>[Task 1] Walk Through on Implementation Details</h4>
			<p>
				In this task, I wrote the code to perform point-in-triangle tests using line equation test. The basic idea is if
				a point is tested as inside the triangle, the code will rasterize it. Also, in order to make the code more
				readable and maintainable, I created several smaller functions to perform different tasks. I'll talk about them
				in details in the following paragraphs.
			</p>
			<p>
				I first created a function <b>RasterizerImp::line_equation_test</b> in class RasterizerImp, which takes three
				2-D coordinates (the first two coordinates form the line, and the third one is the target point to be tested),
				and returns the line equation test result using the formula taught in the lecture
				(<b>-(x - x0) * (y1 - y0) + (y - y0) * (x1 - x0)</b>).
			</p>
			<p>
				I also created another function <b>RasterizerImp::is_on_the_line</b> to test if the absolute value of the line equation test
				result is less than a predefined constant EPSILON_CONSTANT (0.001 in the code). If the absolute value is less
				than EPSILON_CONSTANT, we will consider the point is on the line. We cannot compare against 0 directly due to
				how the floating points are represented in the system.
			</p>
			<p>
			  Another helper function is <b>RasterizerImp::is_same_side</b>. This function takes two line equation test results, and returns
				true of both results have the same sign; returns false otherwise.
			</p>
			<div class="ui warning message">
				<p>
					The tricky part of this task is how to define which side of a line is inside the triangle. During my analysis,
					I found out a pattern:
				</p>
				<p>
					To start with, we can take the line formed by the first two vertexes of the triangle, and get the line test
					result of the third vertex. Then the sign of this line test result can be used as the "target" sign for all points in the canvas.
					When we evaluate each point against the edges of the triangle, we will do so in this particular order:
					edge V1 -> V2, edge V2 -> V3, edge line V3 -> V1. If all 3 line test results of a point are with the same sign
					as the "target" sign, we can consider this point is in the triangle.
				</p>
				<p>
					This will make the code easier to implement and less error-prune.
				</p>
			</div>
			<p>
				Then, I created another function <b>RasterizerImp::is_point_in_triangle</b> to take 1) the coordinates of all 3 vertexes of the
				triangle, 2) the coordinate of the target point, and 3) the target line test result of the third triangle vertex
				as described above. In this function, we will perform at most 3 line tests for the target point. The function will
				return early if 1) the point is on any triangle edge (returns true in this case), or 2) the line test result of
				an edge has different sign with the target line test result (returns false in this case). If all 3 line test results
				are true, we will return true to the caller (in this case, function <b>RasterizerImp::rasterize_triangle</b>).
			</p>
			<p>
				Finally, I can use all above helper functions in <b>RasterizerImp::rasterize_triangle</b>. In this function, I
				use a nested loop to loop through all points on the canvas to get the coordinates of the top-left corner of each pixel.
				For each point, I then calculate the center of the pixel by adding 0.5 to both x and y coordinates. Then, I plug in
				the coordinate of the center of each pixel to function <b>RasterizerImp::is_point_in_triangle</b>.
				If the result is true, <b>RasterizerImp::fill_pixel</b> will be invoked to fill in the sample_buff, which is
				essentially the same as rgb_framebuffer_target in task 1.
			</p>
			<h4>[Task 1] Analysis on Algorithm Efficiency</h4>
			<p>
				In my algorithm, for each point on the canvas, we will need to perform at most 3 line tests. So the worst-case
				time complexity is O(width * height) * 3. Since the constant of O can be omitted when analyzing time complexity,
				we can take the time complexity as O(width * height).
			</p>
			<p>
				This time complexity is the same as the one which checks each sample within the bounding box of the triangle. The
				latter needs to loop through every pixel and evaluate against the bounder in the worst case as well. So its time
				complexity is the same as O(width * height).
			</p>
			<h4>[Task 1] Screenshot for Task 1 ("svg/basic/test4.svg")</h4>
			<img src="./assets/task-1.png">
			
			<h3>Task 2: Antialiasing by Supersampling</h3>
			<h4>[Task 2] Walk Through on Implementation Details</h4>
			<p>
				In this task, I wrote the code to perform antialiasing by supersampling. The high-level idea is that, instead of just
				using the center of each pixel to perform point-in-triangle tests, we "supersample" each pixel by the sample rate,
				and use these supersampled sub-pixels to perform point-in-triangle tests, and then we take the average of the result
				of these "sub-pixels" as the result of the entire pixel. This will reduce jaggies in our rasterized image. But this
				will also consume more computing resources. I'll describe my algorithms and implementation details below.
			</p>
			<p>
				First of all, the sample_buffer vector is the data structure storing all supersampled point-in-triangle tests
				results. Because we have different sample rates now, whenever we need to resize the frame buffer or change the
				sample rate, we need to update the size of our sample_buffer using <b>width * height * sample_rate</b>. This is
				reflected in two functions: <b>RasterizerImp::set_framebuffer_target</b> and <b>RasterizerImp::set_sample_rate</b>.
			</p>
			<p>
				Secondly, in <b>RasterizerImp::rasterize_triangle</b>, we want to calculate the point-in-triangle test for each
				supersampled "sub-pixel"s. To start with, I calculate how many iterations we need to perform for each edge of each
				pixel. This is essentially just <b>sqrt(sample_rate)</b> (variable <b>iter_per_edge</b> in the code) since we
				want to divide each pixel into "sample_rate" umber of "sub-pixel"s. Then I calculate the increment amount we
				need to add to each sub-pixel's coordinate to get the center coordinate.
			</p>
			<p>
				Next, I loop through all original pixels on the canvas, for each pixel, I loop through <b>iter_per_edge</b> on both
				x-axis and y-axis to get the coordinates of each sub-pixel, then plug them in function <b>RasterizerImp::is_point_in_triangle</b>
				(created in Task 1) to get the point-in-triangle test result, which is a Color object. At last, I put them in
				the correctly sizedã€€sample_buffer vector.
			</p>
			<p>
				The last function I modified is <b>RasterizerImp::resolve_to_framebuffer</b>. In this function, I evaluate the <b>sample_buffer</b>
				vector, and resolve the color (r, g, b values in frame buffer) of each original pixel and put them in rgb_framebuffer_target
				array. The resolve logic is: for each original pixel, I instantiate a temporary Color object with value Black
				({ 0, 0, 0 } as rgb values). For each sub-pixel of the original pixel, I add its color to the temporary Color
				object. After the loop, I divide the r, g, b values of the temporary Color object by <b>sample_rate</b> to get the averaged
				result, and use that to fill in the <b>rgb_framebuffer_target</b> array.
			</p>
			<h4>[Task 2] Screenshots for Task 2 ("svg/basic/test4.svg" under different sample rates) (<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">Github Page</a>)</h4>
			<div class="ui three column grid">
				<div class="row">
					<div class="column ui segment">
						<img src="./assets/task-2_sample_rate_1.png">
						<h5 class="ui center aligned container">Sample Rate 1</h5>
					</div>
					<div class="column ui segment">
						<img src="./assets/task-2_sample_rate_4.png">
						<h5 class="ui center aligned container">Sample Rate 4</h5>
					</div>
					<div class="column ui segment">
						<img src="./assets/task-2_sample_rate_16.png">
						<h5 class="ui center aligned container">Sample Rate 16</h5>
					</div>
				</div>
			</div>
			<div class="ui info message">
				<div class="header">[Task 2] Explanation on the Results</div>
				<p>
					We can see from the results: the higher the sample rate is, the fewer jaggies the image has, and the smoother
					the object edge is. This is because we take the average value of many sub-pixels instead of just the center of
					the pixel. The result is much more accurate, especially on edges.
				</p>
				<p>
					Based on the result of this task, we can see supersampling is an effective way to deal with antialiasing.
				</p>
			</div>
			<h3>Task 3: Transforms (<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">Github Page</a>)</h3>
			<h4>My png Image</h4>
			<img src="./assets/task-3.png">
			<div class="ui info message">
				<div class="header">[Task 3] Image Explanation</div>
				<p>
					This png screenshot is based on the <b>svg/transforms/new_robot.svg</b> that I created. In this new svg, I tried
					to rotate the cubeman's left leg clockwise by 90 degree, and the right leg clockwise by 45 degree. So this
					cubeman looks like running. I also use a different color to add a T-shirt for the cubeman.
				</p>
			</div>
			<h3>Task 4: Barycentric coordinates (<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">Github Page</a>)</h3>
			<h4>[Task 4] Barycentric Coordinates Explanation</h4>
			<p>
				Barycentric coordinates is a coordinates system for triangles. Each barycentric coordinate is a 3-D vector. Each
				of the value represents the proportional distance to the corresponding vertex of the triangle. Take the following
				image for example (created based on "svg/basic/triangle.svg" in the project repo)
			<p>
			<img src="./assets/task-4-demo.png">
			<p>
				You can see the color in the triangle is smoothly blended. The points closed to the right vertex has higher "r"
				value, and they look more red; same for the other vertexes.
			</p>
			<h4>[Task 4] Screenshots ("svg/basic/test7.svg")</h4>
			<img src="./assets/task-4.png">
			<p></p>
			
			<h3>Task 5: "Pixel sampling" for texture mapping (<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">Github Page</a>)</h3>
			<h4>[Task 5] Pixel Sampling Explanation and Implementation Details</h4>
			<p>
				Pixel sampling is a texture mapping method to sample the pixels on the texture image to apply the result to the
				screen image. The goal of texture mapping is to make the images on screen more realistic without introducing
				too much resource consuming computer graphics technics (imaging if we don't have texture mapping, we'd need to
				draw these minor details on images to make them to look realistic).
			</p>
			<p>
				The high-level idea of implementing pixel sampling is to translate screen pixel coordinates to texture pixel
				coordinates by using the linear algebra view of barycentric coordinates, and then apply different sampling methods
				to get the sample result in the texture. The translation formula is provided in the lecture notes: essentially,
				we need three points on each of the screen space and texture space, and the 3 * 3 transform matrix can be calculated
				as M_texture * M_screen ^ -1.
			</p>
			<p>
				As for translating each pixel on the screen space to the texture space, I take the coordinate of the pixel on
				the screen, and convert them into a 3-D vector { x, y, 1 }. Then I multiply it by the transform matrix to get
				the barycentric coordinates in the texture space. Inside each sampling function in Texture, I convert the
				barycentric coordinates to the real coordinates by multiplying x by (width - 1), and multiplying y by (height - 1).
				This process can also apply supersampling to get granular coordinates to render better images.
			</p>
			<p>
				I implemented two kinds of pixel sampling: nearest and bilinear. For nearest sampling, once we got the actual
				coordinates on the texture, I just round x and y to the nearest integer and get the color; for bilinear sampling,
				I take the floor and ceil of both x and y, and run linear interpolation 3 times: first two are on the horizontal
				direction (x-axis) to get the color of two endpoints of the vertical line formed by the sample point, the last one
				is on the vertical direction (y-axis) to get the final color of the sample point.
			</p>
			<h4>[Task 5] Screenshots</h4>
			<div class="ui two column grid">
				<div class="row bordered">
					<div class="column ui segment">
						<img src="./assets/task-5_sample_rate_1_nearest.png" title="Sample Rate 1 + Nearest Sampling">
						<h5 class="ui center aligned container">Sample Rate 1 + Nearest Sampling</h5>
					</div>
					<div class="column ui segment">
						<img src="./assets/task-5_sample_rate_1_bilinear.png" title="Sample Rate 1 + Bilinear Sampling">
						<h5 class="ui center aligned container">Sample Rate 1 + Bilinear Sampling</h5>
					</div>
				</div>
				<div class="row">
					<div class="column ui segment">
						<img src="./assets/task-5_sample_rate_16_nearest.png" title="Sample Rate 16 + Nearest Sampling">
						<h5 class="ui center aligned container">Sample Rate 16 + Nearest Sampling</h5>
					</div>
					<div class="column ui segment">
						<img src="./assets/task-5_sample_rate_16_bilinear.png" title="Sample Rate 16 + Bilinear Sampling">
						<h5 class="ui center aligned container">Sample Rate 16 + Bilinear Sampling</h5>
					</div>
				</div>
			</div>
			<div class="ui info message">
				<div class="header">[Task 5] Comment on the Comparison Result</div>
				<div class="list">
					<li>
						Based on the result, bilinear sampling renders better results than nearest sampling. But the advantage of
						bilinear sampling diminishes with increased sample rate. This is because with higher sample rate, for bilinear
						sampling, the floor and ceil endpoints will become very close to each other, and the linear interpolation will
						get similar value to either one of these two endpoints, which is essentially the same as nearest sampling
						in practice. And it is also worth noting that bilinear sampling costs more computing power.
					</li>
					<li>
						We can also see from the results that, generally speaking, the higher the sample rate we use, the smoother
						the image we will get.
					</li>
					<li>
						To summarize, for low sample rate, bilinear sampling can get better image than nearest sampling; but for high
						sample rate, the results between these two approaches are similar, but nearest sampling costs less computing
						resource.
					</li>
				</div>
			</div>
			<h3>Task 6: "Level sampling" with mipmaps for texture mapping (<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">Github Page</a>)</h3>
			<h4>[Task 6] Level Sampling Explanation and Implementation Details</h4>
			<p>
				Level sampling is to use pre-computed filtered version of the texture (also called MIP Maps) to perform texture
				mapping. This technique is especially useful when multiple texels can contribute to one single pixel footprint
				in the screen space (caused by perspective projections). In this case, pixel sampling can be expensive for a very small
				area in the screen space (if we calculate the average of all covered texels in the texture space). Level sampling
				is to solve this issue. In this technique, we pre-compute different levels of filtered version of the texture,
				and store them in a data structure called MIP Map. Depending on how fast the mapped texture footprint changes
				(we can use derivative), we will select the corresponding levels to perform sampling. The faster the texture
				footprint increases, the higher level we should use. This will significantly reduce compute time comparing to
				pixel sampling.
			</p>
			<p>
				As for implementation details, I first created function <b>RasterizerImp::get_barycentric_uv</b> to calculate
				barycentric coordinates (in 2-D vector format) for any given screen space coordinates. Then, I use this function
				to calculate barycentric coordinates for (x, y), (x + 1, y), (x, y + 1) for any given pixel in the screen space.
				At last, I construct the <b>SampleParams</b> object using these three calculated 2-D vectors with pixel sample
				method (psm) and level sample method (lsm), and pass it in to the Texture object to calculate the corresponding
				MIP map level.
			</p>
			<p>
				In function <b>Texture::get_level</b>, I calculate the dx and dy vectors by subtracting p_dx_uv and p_dy_uv
				with p_uv, respectively. Then I multiply these 2 vectors by the texture width and height to get the actual
				coordinates in the texture space (named as <b>dx_scaled</b> and <b>dy_scaled</b> in the code). Then, I calculate
				the MIP map level by formula <b>log2(max(dx_scaled.norm(), dy_scaled.norm()))</b>. This is basically how fast
				the mapped texture footprint changes. If the level sample method is L_NEAREST, the function rounds the result to
				the nearest integer; if the level sample method is L_ZERO, it returns 0 directly; if the level sample method is
				L_LINEAR, it returns the float level value as is and handle it in <b>RasterizerImp</b> (described in the next
				paragraph).
			</p>
			<p>
				Once we get the level value back from <b>Texture::get_level</b>, we will then use it resolve the color of the
				pixel in the screen space. If the level sample method isn't L_LINEAR, we will just take the sample from the
				texture using the level directly (nearest or linear based on pixel sample method). If the level sampling method
				is L_LINEAR, then we need to perform linear interpolation on the color from the floor level and the color from
				the ceil level.
			</p>
			<h4>[Task 6] Tradeoffs among different sampling techniques</h4>
			<p>
				First of all, supersampling with high sample rate definitely consumes the most memory, because it needs to
				allocate the sample buffer by multiplying the sample rate. But the advantage of high sample rate is also obvious,
				it usually results in better images.
			</p>
			<p>
				Bilinear pixel sampling renders better image comparing to nearest pixel sampling. But that's with the cost of
				speed due to linear interpolation. Also the better image argument for bilinear pixel sampling diminishes with high sample
				rate.
			</p>
			<p>
				As for level sampling, nearest and bilinear sampling methods generate better images comparing to zero level
				sampling. But the speed of these two is slower. Bilinear level sampling with bilinear pixel sampling (also
				called trilinear sampling) is the slowest combination (under the same sample rate) due to multiple round of
				linear interpolation for each pixel. But usually, it produces the best result.
			</p>
			<h4>[Task 6] Screenshots (my own png image with "svg/texmap/test6.svg" in the project repo)</h4>
			<div class="ui two column grid">
				<div class="row bordered">
					<div class="column ui segment">
						<img src="./assets/task-6_l-zero_p-nearest.png" title="L_ZERO + P_NEAREST">
						<h5 class="ui center aligned container">L_ZERO + P_NEAREST</h5>
					</div>
					<div class="column ui segment">
						<img src="./assets/task-6_l-zero_p-bilinear.png" title="L_ZERO + P_LINEAR">
						<h5 class="ui center aligned container">L_ZERO + P_LINEAR</h5>
					</div>
				</div>
				<div class="row">
					<div class="column ui segment">
						<img src="./assets/task-6_l-nearest_p-nearest.png" title="L_NEAREST + P_NEAREST">
						<h5 class="ui center aligned container">L_NEAREST + P_NEAREST</h5>
					</div>
					<div class="column ui segment">
						<img src="./assets/task-6_l-nearest_p-linear.png" title="L_NEAREST + P_LINEAR">
						<h5 class="ui center aligned container">L_NEAREST + P_LINEAR</h5>
					</div>
				</div>
			</div>
			<div class="ui info message">
				<div class="header">[Task 6] Comment on the Comparison Result</div>
				<div class="list">
					<li>
						The combination of L_ZERO + P_NEAREST produces the worst textured image. But it's the fastest.
					</li>
					<li>
						P_LINEAR can reduce the jaggies efficiently under this sample rate (1).
					</li>
					<li>
						The combination of L_NEAREST + P_LINEAR produces the best image, it's also the slowest one among these 4.
					</li>
				</div>
			</div>
			<br><br>
			<div class="ui right aligned container">
				<h5>
					Live version of this write-up is hosted at
					<a href="https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html">
						https://cal-cs184-student.github.io/project-webpages-sp23-xyuan195/proj1/index.html
					</a>
				</h5>
				<br><br>
			</div>
		</div>
	</body>
</html>